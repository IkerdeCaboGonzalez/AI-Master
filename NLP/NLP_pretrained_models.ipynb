{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BifFccBs8JEh",
        "outputId": "d1ce7880-745a-41a3-c582-80ba452f0df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# Importamos pipeline y un tokenizador para usar los modelos de procesamiento de lenguaje natural\n",
        "!pip install transformers\n",
        "from transformers import pipeline, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto_entrada= \"\"\"La tecnología ha revolucionado prácticamente todos los aspectos de nuestras vidas, desde cómo nos\n",
        "comunicamos hasta cómo realizamos nuestras tareas diarias, cambiando también la manera en que\n",
        "interactuamos con el mundo que nos rodea. Su impacto es evidente en áreas tan diversas como la\n",
        "educación, la salud, el transporte, y el comercio, mejorando la eficiencia, la accesibilidad y la\n",
        "conectividad a niveles nunca antes imaginados. Sin embargo, a pesar de los innumerables beneficios que\n",
        "nos brinda, la tecnología también trae consigo desafíos complejos y significativos que no podemos\n",
        "ignorar.\n",
        "Uno de los retos más destacados es la brecha digital, que exacerba las desigualdades sociales y\n",
        "económicas entre quienes tienen acceso a las tecnologías y quienes no. Esta disparidad limita las\n",
        "oportunidades educativas, laborales y sociales de muchas personas en diversas partes del mundo,\n",
        "dejando atrás a comunidades enteras en el desarrollo tecnológico. Además, la expansión de la tecnología\n",
        "ha planteado serias preocupaciones en torno a la privacidad y la seguridad de los datos personales, ya\n",
        "que gran parte de nuestras actividades diarias, desde transacciones bancarias hasta interacciones\n",
        "sociales, dejan un rastro digital que puede ser explotado.\n",
        "Es crucial reflexionar detenidamente sobre estos aspectos para mitigar los riesgos asociados al avance\n",
        "tecnológico y garantizar que su impacto sea positivo para la mayor cantidad de personas posible. Sólo a\n",
        "través de un análisis consciente y acciones responsables podemos aspirar a construir un futuro más\n",
        "equilibrado, donde la tecnología sea una herramienta al servicio de toda la humanidad, en lugar de\n",
        "convertirse en una fuente de desigualdad o amenaza.\"\"\""
      ],
      "metadata": {
        "id": "zgQZ4t108QGi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos el texto y creamos el pipeline para el modelo de resumen\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert2bert_shared-spanish-finetuned-summarization\")\n",
        "\n",
        "resumen_pipeline = pipeline(\"summarization\", model=\"mrm8488/bert2bert_shared-spanish-finetuned-summarization\",  tokenizer=tokenizer)\n",
        "\n",
        "# Aplicamos el modelo al texto\n",
        "\n",
        "resumen=resumen_pipeline(texto_entrada, min_length=100, max_length=200, do_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TLHiteIjjVdB",
        "outputId": "e181ac6e-9d47-471e-fa76-2ac6706f6fe7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Config of the encoder: <class 'transformers.models.bert.modeling_bert.BertModel'> is overwritten by shared encoder config: BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.bert.modeling_bert.BertLMHeadModel'> is overwritten by shared decoder config: BertConfig {\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el pipeline para la traducción\n",
        "\n",
        "traduccion_pipeline=pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Lo aplicamos sobre el texto original\n",
        "\n",
        "traduccion=traduccion_pipeline(texto_entrada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5UIxvQBaFkC",
        "outputId": "8496a5db-694b-40df-cc8d-e91bc00837bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el pipeline para el análisis de sentimientos\n",
        "\n",
        "sentimiento_pipeline=pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
        "\n",
        "# Lo aplicamos sobre el texto\n",
        "\n",
        "sentimiento=sentimiento_pipeline(texto_entrada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPmTRpeJaI3G",
        "outputId": "cfee8238-91ff-4b61-e7c4-35282e842973"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Resultados\n",
        "\n",
        "print('resumen: ', resumen[0]['summary_text'])\n",
        "\n",
        "print('traduccion: ', traduccion[0]['translation_text'])\n",
        "\n",
        "print('sentimiento: ', sentimiento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOsXg_Vz4ELa",
        "outputId": "e287ad59-3ddb-4cc0-eadf-ad4f1ee45190"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resumen:  La brecha digital, que exacerba las desigualdades sociales y económicas entre quienes tienen acceso a las tecnologías y quienes no pueden acceder a las nuevas tecnologías, es una de las principales preocupaciones de las personas en distintas partes del mundo, desde cómo nos comunicamos hasta cómo interactuamos con el mundo que nos rodea, desde a través de un análisis consciente y acciones responsables, a cómo realizamos nuestras tareas diarias y sociales de todas las personas o quienes no nos sometemos a la realidad y a que no podemos ignorar\n",
            "traduccion:  Technology has revolutionized virtually every aspect of our lives, from how we communicate to how we perform our daily tasks, also changing the way we interact with the world around us. Its impact is evident in areas as diverse as education, health, transport, and trade, improving efficiency, accessibility and connectivity at levels never before imagined. However, despite the countless benefits it brings us, technology also brings with it complex and significant challenges that we cannot ignore. One of the most important challenges is the digital divide, which exacerbates social and economic inequalities between those who have access to technologies and those who do not. This disparity limits the educational, employment and social opportunities of many people in various parts of the world, leaving entire communities behind in technological development. In addition, the expansion of technology has raised serious concerns about privacy and the security of personal data, since much of our daily activities, from banking transactions to social interactions, leaves a digital trail that can be exploited. It is crucial to reflect carefully on these aspects to mitigate the risks associated with technological advancement and to ensure that their impact is positive for the greatest number of people, to be able to build a conscious tool.\n",
            "sentimiento:  [{'label': 'NEG', 'score': 0.6361498832702637}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a analizar brevemente los resultados obtenidos. El resumen es algo incoherente, y se salta algunos conceptos importantes del texto original. La traducción por otro lado es muy buena, y el análisis de los sentimientos nos da un resultado negativo, con una confianza no demasiado grande en el resultado. Este resultado del análisis de sentimientos es lógico, ya que el texto es una especie de crítica y aviso acerca de los problemas y retos que presenta las nuevas tecnologías, aunque también recalca que tiene unos grandes beneficios, de ahi seguramente la incertidumbre del modelo acerca del resultado negativo. En conclusión obtenemos un resultado mucho mejor en traducción y análisis de sentimientos comparado con el resumen. Esto parece lógico, teniendo en cuenta que la traducción es un proceso más mecánico, donde solo debe cambiar las palabras de un idioma a otro, y al no existir en este texto formas de comunicación más complejas como bromas, ironías o referencias a la cultura particular de cada idioma, lo que puede resultar más complicado de traducir, el modelo hace un trabajo excelente. Sin embargo, en el resumen, donde es necesario sintetizar y reformular la información del texto original, el modelo (se han probado varios, con resultados aún peores que el expuesto) tiene un desempeño considerablemente más pobre."
      ],
      "metadata": {
        "id": "juhGurxAu760"
      }
    }
  ]
}